{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65035756-20ca-4573-8489-70e4b976ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'titanic'\n",
    "reduced = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f8fade-cfaf-41f0-a242-9a07c973a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043d897-b064-437c-9f7f-334c6b5f96bd",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accredited-occasions",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-4189ff44d84d>:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data_with_labels['name'][i] = re.sub('\"', '', name)\n",
      "<ipython-input-4-4189ff44d84d>:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tit_test['Name'][i] = re.sub('\"', '', name)\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'titanic':\n",
    "    #https://www.kaggle.com/c/titanic/data\n",
    "    tit_sub = pd.read_csv('./data/Titanic/gender_submission.csv')\n",
    "    tit_train = pd.read_csv('./data/Titanic/train.csv')\n",
    "    tit_test = pd.read_csv('./data/Titanic/test.csv')\n",
    "    df_train_final = pd.read_pickle(\"./data/Titanic/df_train_final\")\n",
    "    df_test_final = pd.read_pickle(\"./data/Titanic/df_test_final\")\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler_cols = ['Age', 'Fare', 'Name_Length', 'Family_Size', 'Name_Length', 'Ticket_Frequency', 'Fare_Family_Size', 'Fare_Cat_Pclass']\n",
    "    std = StandardScaler()\n",
    "    std.fit(df_train_final[scaler_cols])\n",
    "    df_train_final.loc[:, scaler_cols] = std.transform(df_train_final[scaler_cols])\n",
    "    df_test_final.loc[:, scaler_cols] = std.transform(df_test_final[scaler_cols])\n",
    "\n",
    "    features = ['Pclass', 'Sex', 'Age', 'Fare', 'Title', 'Name_Length', 'Emb_C',\n",
    "           'Emb_Q', 'Emb_S', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs',\n",
    "           'Title_Other', 'Title_Royal', 'Family_Size',\n",
    "           'Family_Friends_Surv_Rate', 'Cabin_Clean',\n",
    "           'Ticket_Frequency', 'Tkt_AS', 'Tkt_C', 'Tkt_CA',\n",
    "           'Tkt_CASOTON', 'Tkt_FC', 'Tkt_FCC', 'Tkt_Fa', 'Tkt_LINE', \n",
    "           'Tkt_NUM', 'Tkt_PC', 'Tkt_PP', 'Tkt_PPP', 'Tkt_SC', 'Tkt_SCA',\n",
    "           'Tkt_SCAH', 'Tkt_SCAHBasle', 'Tkt_SCOW', 'Tkt_SCPARIS', 'Tkt_SCParis',\n",
    "           'Tkt_SOC', 'Tkt_SOP', 'Tkt_SOPP', 'Tkt_SOTONO', 'Tkt_SOTONOQ', 'Tkt_SP',\n",
    "           'Tkt_STONO', 'Tkt_SWPP', 'Tkt_WC', 'Tkt_WEP', 'Fare_Cat', 'Child', 'Senior']\n",
    "\n",
    "    features_train = ['Survived', 'Pclass', 'Sex', 'Age', 'Fare', 'Title', 'Name_Length', 'Emb_C',\n",
    "           'Emb_Q', 'Emb_S', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs',\n",
    "           'Title_Other', 'Title_Royal', 'Family_Size',\n",
    "           'Family_Friends_Surv_Rate', 'Cabin_Clean',\n",
    "           'Ticket_Frequency', 'Tkt_AS', 'Tkt_C', 'Tkt_CA',\n",
    "           'Tkt_CASOTON', 'Tkt_FC', 'Tkt_FCC', 'Tkt_Fa', 'Tkt_LINE',\n",
    "           'Tkt_NUM', 'Tkt_PC', 'Tkt_PP', 'Tkt_PPP', 'Tkt_SC', 'Tkt_SCA',\n",
    "           'Tkt_SCAH', 'Tkt_SCAHBasle', 'Tkt_SCOW', 'Tkt_SCPARIS', 'Tkt_SCParis',\n",
    "           'Tkt_SOC', 'Tkt_SOP', 'Tkt_SOPP', 'Tkt_SOTONO', 'Tkt_SOTONOQ', 'Tkt_SP',\n",
    "           'Tkt_STONO', 'Tkt_SWPP', 'Tkt_WC', 'Tkt_WEP', 'Fare_Cat', 'Child', 'Senior']\n",
    "\n",
    "    df_train_final = df_train_final[features_train]\n",
    "    df_test_final = df_test_final[features]\n",
    "\n",
    "    # train/test/val split\n",
    "    features = df_test_final.columns.to_list()\n",
    "    X_train = df_train_final[features]\n",
    "    Y_train = df_train_final['Survived']\n",
    "    X_test = df_test_final.reset_index(drop=True)\n",
    "\n",
    "    c=pd.read_csv('./data/Titanic/titanic_test_labels.csv')\n",
    "    test_data_with_labels = c.copy()\n",
    "    for i, name in enumerate(test_data_with_labels['name']):\n",
    "        if '\"' in name:\n",
    "            test_data_with_labels['name'][i] = re.sub('\"', '', name)\n",
    "    for i, name in enumerate(tit_test['Name']):\n",
    "        if '\"' in name:\n",
    "            tit_test['Name'][i] = re.sub('\"', '', name)\n",
    "    survived = []\n",
    "    for name in tit_test['Name']:\n",
    "        survived.append(int(test_data_with_labels.loc[test_data_with_labels['name'] == name]['survived'].values[-1]))\n",
    "    Y_test = pd.Series(survived,index=X_test.index)\n",
    "\n",
    "    Y_train = Y_train.to_numpy()\n",
    "    Y_test = Y_test.to_numpy()\n",
    "elif dataset == 'adult':\n",
    "    train_df = pd.read_csv('./data/Adult/adult.data',\n",
    "                           header=None,\n",
    "                           names=['age','workclass','fnlwgt','education','education_num','marital_status','occupation',\n",
    "                                  'relationship','race','sex','capital_gain','capital_loss','hours_per_week','native_country','target'])\n",
    "    test_df = pd.read_csv('./data/Adult/adult.test',\n",
    "                           header=0,\n",
    "                           names=['age','workclass','fnlwgt','education','education_num','marital_status','occupation',\n",
    "                                  'relationship','race','sex','capital_gain','capital_loss','hours_per_week','native_country','target'])\n",
    "\n",
    "    train_df = train_df[np.sum(train_df.values==' ?',axis=1)==0]\n",
    "    test_df = test_df[np.sum(test_df.values==' ?',axis=1)==0]\n",
    "\n",
    "    train_df = train_df.drop(columns='education_num')\n",
    "    test_df = test_df.drop(columns='education_num')\n",
    "    \n",
    "    from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "    categorical_columns = ['workclass','education','marital_status','occupation','relationship','race','sex','native_country']\n",
    "\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    train_df[enc.get_feature_names()] = enc.fit_transform(train_df.loc[:,categorical_columns]).astype(int).toarray()\n",
    "    test_df[enc.get_feature_names()] = enc.transform(test_df.loc[:,categorical_columns]).astype(int).toarray()\n",
    "\n",
    "    ordinal_columns = ['target']\n",
    "\n",
    "    enc = OrdinalEncoder()\n",
    "    train_df.loc[:,ordinal_columns] = enc.fit_transform(train_df.loc[:,ordinal_columns].values).astype(int)\n",
    "    test_df.loc[:,ordinal_columns] = enc.transform(test_df.loc[:,ordinal_columns].values).astype(int)\n",
    "\n",
    "    train_df = train_df.drop(columns=categorical_columns)\n",
    "    test_df = test_df.drop(columns=categorical_columns)\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler_cols = ['age', 'fnlwgt', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "    std = MinMaxScaler()\n",
    "    std.fit(train_df.loc[:,scaler_cols])\n",
    "    train_df.loc[:, scaler_cols] = std.transform(train_df.loc[:,scaler_cols])\n",
    "    test_df.loc[:, scaler_cols] = std.transform(test_df.loc[:,scaler_cols])\n",
    "    \n",
    "    X_train = train_df.drop('target',axis=1)\n",
    "    Y_train = train_df.loc[:,'target']\n",
    "    X_test = test_df.drop('target',axis=1)\n",
    "    Y_test = test_df.loc[:,'target']\n",
    "    \n",
    "    \n",
    "else:\n",
    "    raise Exception('dataset name not recognized, only adult or titanic are supported')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-vanilla",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "diagnostic-pacific",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN_VAE(\n",
      "  (fc1): Linear(in_features=51, out_features=24, bias=True)\n",
      "  (drop1): Dropout(p=0, inplace=False)\n",
      "  (fc2): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (fc3_mu): Linear(in_features=12, out_features=2, bias=True)\n",
      "  (fc3_logvar): Linear(in_features=12, out_features=2, bias=True)\n",
      "  (fc4): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=12, bias=True)\n",
      "    (1): Dropout(p=0, inplace=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc5): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=24, bias=True)\n",
      "    (1): Dropout(p=0, inplace=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc6): Linear(in_features=24, out_features=51, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "if dataset == 'titanic':\n",
    "    class FFNN_VAE(nn.Module):\n",
    "        def __init__(self, input_shape, hidden, dropout_p=0.3, latent_dim=2):\n",
    "            super(FFNN_VAE, self).__init__()\n",
    "\n",
    "            # encoding components\n",
    "\n",
    "            self.fc1 = nn.Linear(input_shape,hidden[0])\n",
    "            self.drop1 = nn.Dropout(dropout_p)\n",
    "            self.fc2 = nn.Linear(hidden[0],hidden[1])\n",
    "            # Latent vectors mu and sigma\n",
    "            self.fc3_mu = nn.Linear(hidden[1], latent_dim)      \n",
    "            self.fc3_logvar = nn.Linear(hidden[1], latent_dim)  \n",
    "\n",
    "            # Sampling vector\n",
    "            self.fc4 = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden[1]),\n",
    "                nn.Dropout(dropout_p),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            # Decoder\n",
    "            self.fc5 = nn.Sequential(\n",
    "                nn.Linear(hidden[1], hidden[0]),\n",
    "                nn.Dropout(dropout_p),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            self.fc6 = nn.Linear(hidden[0], input_shape)\n",
    "\n",
    "        def encode(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(self.drop1(x)))\n",
    "            mu, logvar = self.fc3_mu(x), self.fc3_logvar(x)\n",
    "            return mu, logvar\n",
    "\n",
    "        def reparameterize(self, mu, logvar):\n",
    "            if self.training:\n",
    "                std = logvar.mul(0.5).exp_()\n",
    "                eps = Variable(std.data.new(std.size()).normal_())\n",
    "                return eps.mul(std).add_(mu)\n",
    "            else:\n",
    "                return mu\n",
    "\n",
    "        def decode(self, z):\n",
    "            x = self.fc4(z)\n",
    "            x = self.fc5(x)\n",
    "            x = self.fc6(x)\n",
    "            return x\n",
    "\n",
    "        def forward(self, x):\n",
    "            mu, logvar = self.encode(x)\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            x_reconst = self.decode(z)\n",
    "            return x_reconst, z, mu, logvar\n",
    "\n",
    "    # hyperparameters\n",
    "    latent_dim = 2    # latent dim extracted by 2D CNN\n",
    "    dropout_p = 0       # dropout probability\n",
    "    hidden = [24,12]\n",
    "\n",
    "elif dataset == 'adult':\n",
    "    class FFNN_VAE(nn.Module):\n",
    "        def __init__(self, input_shape, hidden, latent_dim=2, dropout_p=0):\n",
    "            super(FFNN_VAE, self).__init__()\n",
    "\n",
    "            # encoding components\n",
    "\n",
    "            self.fc1 = nn.Linear(input_shape, hidden[0])\n",
    "            self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "            self.fc3 = nn.Linear(hidden[1], hidden[2])\n",
    "            # Latent vectors mu and sigma\n",
    "            self.fc4_mu = nn.Linear(hidden[2], latent_dim)      \n",
    "            self.fc4_logvar = nn.Linear(hidden[2], latent_dim) \n",
    "\n",
    "            # Sampling vector\n",
    "            self.fc5 = nn.Linear(latent_dim, hidden[2])\n",
    "            # Decoder\n",
    "            self.fc6 = nn.Linear(hidden[2], hidden[1])\n",
    "            self.fc7 = nn.Linear(hidden[1], hidden[0])\n",
    "            self.fc8 = nn.Linear(hidden[0], input_shape)\n",
    "\n",
    "        def encode(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            mu, logvar = self.fc4_mu(x), self.fc4_logvar(x)\n",
    "            return mu, logvar\n",
    "\n",
    "        def reparameterize(self, mu, logvar):\n",
    "            if self.training:\n",
    "                std = logvar.mul(0.5).exp_()\n",
    "                eps = Variable(std.data.new(std.size()).normal_())\n",
    "                return eps.mul(std).add_(mu)\n",
    "            else:\n",
    "                return mu\n",
    "\n",
    "        def decode(self, z):\n",
    "            x = F.relu(self.fc5(z))\n",
    "            x = F.relu(self.fc6(x))\n",
    "            x = F.relu(self.fc7(x))\n",
    "            x = self.fc8(x)\n",
    "            return x\n",
    "\n",
    "        def forward(self, x):\n",
    "            mu, logvar = self.encode(x)\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            x_reconst = self.decode(z)\n",
    "            return x_reconst, z, mu, logvar\n",
    "\n",
    "    # hyperparameters\n",
    "    latent_dim = 2    \n",
    "    hidden = [50,25,12]\n",
    "    dropout_p = 0\n",
    "    \n",
    "    \n",
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   \n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")   \n",
    "\n",
    "# Create Model\n",
    "FFNN_VAE_model = FFNN_VAE(len(X_train.columns),hidden = hidden, dropout_p=dropout_p,latent_dim=latent_dim)\n",
    "print(FFNN_VAE_model)\n",
    "\n",
    "if dataset == 'titanic':\n",
    "    FFNN_VAE_model.load_state_dict(torch.load('./models/VAE_Titanic.pt'))\n",
    "elif dataset == 'adult':\n",
    "    FFNN_VAE_model.load_state_dict(torch.load('./models/VAE_Adult.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "optional-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    FFNN_VAE_model.eval()\n",
    "    rec_x_train, z_train, mu, logvar = FFNN_VAE_model(torch.tensor(X_train.to_numpy(),dtype=torch.float32))\n",
    "    rec_x_test, z_test, mu, logvar = FFNN_VAE_model(torch.tensor(X_test.to_numpy(),dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9edb0a-ee5c-4220-a1de-f00925780021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e15045-0171-49a3-9a8c-dea09aeb8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'adult':\n",
    "    reduced = np.random.permutation(range(15059))[:1000]\n",
    "    X_test = X_test.iloc[reduced]\n",
    "    X_test.reset_index(drop=True)\n",
    "    Y_test = Y_test.iloc[reduced]\n",
    "    Y_test.reset_index(drop=True)\n",
    "    z_test = z_test[reduced]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-andrew",
   "metadata": {},
   "source": [
    "# APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "actual-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "x_min, x_max = z_test[:, 0].min() - 0.1, z_test[:, 0].max() + 0.1\n",
    "if dataset == 'titanic':\n",
    "    y_min, y_max = z_test[:, 1].min() - 1, z_test[:, 1].max() + 1\n",
    "elif dataset == 'adult':\n",
    "    y_min, y_max = z_test[:, 1].min() - 0.1, z_test[:, 1].max() + 0.1\n",
    "\n",
    "fig = go.Figure()\n",
    "colorscale = [[0, '#3b4cc0'],[1, '#b40426']]\n",
    "\n",
    "if dataset == 'titanic':\n",
    "    target_name = 'dead'\n",
    "elif dataset == 'adult':\n",
    "    target_name = '<50K'\n",
    "\n",
    "fig.add_trace(go.Scatter(x=z_test[:, 0].numpy(), y=z_test[:, 1].numpy(),mode='markers',name=target_name,\n",
    "                         marker=dict(\n",
    "                             size=10,\n",
    "                             symbol='circle',\n",
    "                             color=Y_test, \n",
    "                             opacity=0.5,\n",
    "                             colorscale=colorscale,\n",
    "                             line=dict(width=1,\n",
    "                                       color='Black'))))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=[0], y=[0] ,mode='markers',name='pointer',showlegend=False,\n",
    "                         marker=dict(\n",
    "                             size=30,\n",
    "                             symbol='x',\n",
    "                             color='black')))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=[0], y=[0] ,mode='markers',name='expected_value',showlegend=False,\n",
    "                         marker=dict(\n",
    "                             size=15,\n",
    "                             symbol='triangle-up',\n",
    "                             color='purple')))\n",
    "\n",
    "fig.update_yaxes(range=[y_min, y_max])\n",
    "fig.update_xaxes(range=[x_min, x_max])\n",
    "names = ['vector_0','vector_1','vector_2','vector_3','vector_4','vector_5','vector_6','vector_7','vector_8','vector_9']\n",
    "\n",
    "annotations = [\n",
    "    go.layout.Annotation(\n",
    "            #start\n",
    "            x=0, \n",
    "            y=0,\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            #end\n",
    "            ax=0, \n",
    "            ay=0,\n",
    "            axref = \"x\", \n",
    "            ayref = \"y\",\n",
    "            showarrow=True,\n",
    "            arrowside='start',\n",
    "            arrowhead=2, # type od head [0,8]\n",
    "            arrowsize=1, # head dimension \n",
    "            arrowwidth=2, # arrow dimension\n",
    "            arrowcolor=\"#000000\",\n",
    "            name=name,\n",
    "            text='  ',\n",
    "            hovertext='baseline',\n",
    "            font=dict(\n",
    "                family=\"Courier New, monospace\",\n",
    "                size=1,\n",
    "                color=\"#ffffff\"\n",
    "            ),\n",
    "            bgcolor='#ff7f0e'\n",
    "            ) for name in names]+[\n",
    "     go.layout.Annotation(\n",
    "            #start\n",
    "            x=0, \n",
    "            y=0,\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            #end\n",
    "            ax=0, \n",
    "            ay=0,\n",
    "            axref = \"x\", \n",
    "            ayref = \"y\",\n",
    "            showarrow=True,\n",
    "            arrowside='start',\n",
    "            arrowhead=2, # type od head [0,8]\n",
    "            arrowsize=1, # head dimension \n",
    "            arrowwidth=2, # arrow dimension\n",
    "            arrowcolor=\"#000000\",\n",
    "            name='others_contrib',\n",
    "            text='  ',\n",
    "            hovertext='other contributions',\n",
    "            font=dict(\n",
    "                family=\"Courier New, monospace\",\n",
    "                size=1,\n",
    "                color=\"#ffffff\"\n",
    "            ),\n",
    "            bgcolor='#ff7f0e'\n",
    "            )\n",
    "]\n",
    "\n",
    "fig.update_layout(\n",
    "    {'xaxis':{'range':[x_min,x_max]},\n",
    "     'yaxis':{'range':[y_min,y_max]}},\n",
    "    showlegend=True,\n",
    "    autosize=True,\n",
    "    annotations=annotations,\n",
    "    #width=1000,\n",
    "    #height=500,\n",
    "    margin=dict(\n",
    "    l=5,\n",
    "    r=0,\n",
    "    b=5,\n",
    "    t=10,\n",
    "    pad=0\n",
    "));\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unique-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "def wrapper(X):\n",
    "    FFNN_VAE_model.eval()\n",
    "    mu, log_var = FFNN_VAE_model.encode(torch.tensor(X).float())\n",
    "    return mu.detach().numpy()\n",
    "\n",
    "\n",
    "# use Kernel SHAP to explain test set predictions\n",
    "explainer = shap.KernelExplainer(wrapper, shap.sample(X_train,100), link=\"identity\")\n",
    "\n",
    "#    data : numpy.array or pandas.DataFrame or shap.common.DenseData or any scipy.sparse matrix\n",
    "#        The background dataset to use for integrating out features. To determine the impact\n",
    "#        of a feature, that feature is set to \"missing\" and the change in the model output\n",
    "#        is observed. Since most models aren't designed to handle arbitrary missing data at test\n",
    "#        time, we simulate \"missing\" by replacing the feature with the values it takes in the\n",
    "#        background dataset. So if the background dataset is a simple sample of all zeros, then\n",
    "#        we would approximate a feature being missing by setting it to zero. For small problems\n",
    "#        this background dataset can be the whole training set, but for larger problems consider\n",
    "#        using a single reference value or using the kmeans function to summarize the dataset.\n",
    "#        Note: for sparse case we accept any sparse matrix but convert to lil format for\n",
    "#        performance.\n",
    "#    link : \"identity\" or \"logit\"\n",
    "#        A generalized linear model link to connect the feature importance values to the model\n",
    "#        output. Since the feature importance values, phi, sum up to the model output, it often makes\n",
    "#        sense to connect them to the output with a link function where link(output) = sum(phi).\n",
    "#        If the model output is a probability then the LogitLink link function makes the feature\n",
    "#        importance values have log-odds units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "modern-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "mins = np.min(X_train.values,axis=0)\n",
    "maxs = np.max(X_train.values,axis=0)\n",
    "selected_point = 0\n",
    "change_point = 0\n",
    "query = X_test.values[selected_point,:]\n",
    "exp_input_values = X_test.mean(axis=0).values\n",
    "\n",
    "# main function to output the xai scores\n",
    "def compute_XAI_values(query):\n",
    "    \n",
    "    #original_shap\n",
    "    shap_values = explainer.shap_values(query, nsamples='auto')\n",
    "    return explainer.expected_value[0], shap_values[0] ,explainer.expected_value[1], shap_values[1]\n",
    "    \n",
    "\n",
    "exp_x, xai_x, exp_y, xai_y = compute_XAI_values(query)\n",
    "fig.for_each_trace(\n",
    "        lambda trace: trace.update(x=[exp_x], y=[exp_y]) if trace.name == \"expected_value\" else (),\n",
    "        )\n",
    "\n",
    "s = [exp_x, exp_y]\n",
    "#select only the 10 most important features\n",
    "indices = np.argsort(np.sqrt(xai_x**2+xai_y**2))[-10:][::-1]\n",
    "# list of the increments of the sliders\n",
    "if dataset == 'titanic':\n",
    "    steps = [1,1,0.001,0.001,1,0.001,1,1,1,1,1,1,1,1,1,0.001,0.001,1,0.001,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "elif dataset == 'adult':\n",
    "    steps=[0.1]*5+[1]*98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "optional-button",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:8090/\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import dash\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash.exceptions import PreventUpdate\n",
    "\n",
    "# function to select the ticks of the sliders\n",
    "def select_ticks(m, M, x, e):\n",
    "    # round values\n",
    "    if x % 1:\n",
    "        x = np.round(x,3)\n",
    "    else: x = int(x)\n",
    "    if m % 1:\n",
    "        m = np.round(m,3)\n",
    "    else: m = int(m)\n",
    "    if M % 1:\n",
    "        M = np.round(M,3)\n",
    "    else: M = int(M)\n",
    "    if e%1:\n",
    "        e = np.round(e,3)\n",
    "    else: e = int(e)\n",
    "\n",
    "    if x==m:\n",
    "        ticks = dict(sorted({e:{'label':'e','style': {'color': '#800081'}},\n",
    "                    x:{'label':'x','style': {'color': '#000000'}},\n",
    "                    M:{'label':str(M),'style': {'color': '#ed4731'}}\n",
    "                    }.items()))\n",
    "    elif x==M:\n",
    "        ticks = dict(sorted({m:{'label':str(m), 'style': {'color': '#77b0b1'}},\n",
    "                    e:{'label':'e','style': {'color': '#800081'}},\n",
    "                    x:{'label':str(x),'style': {'color': '#000000'}}\n",
    "                    }.items()))\n",
    "    else:\n",
    "        ticks = dict(sorted({m:{'label':str(m), 'style': {'color': '#77b0b1'}},\n",
    "                    e:{'label':'e','style': {'color': '#800081'}},\n",
    "                    x:{'label':str(x),'style': {'color': '#000000'}},\n",
    "                    M:{'label':str(M),'style': {'color': '#ed4731'}}\n",
    "                    }.items()))\n",
    "    return ticks\n",
    "\n",
    "app = JupyterDash(__name__,meta_tags=[{\"name\": \"viewport\", \"content\": \"width=device-width\"}])\n",
    "\n",
    "app.layout = \\\n",
    "html.Div([\n",
    "    html.Div([\n",
    "        html.H3('Latent Space Explanation'),\n",
    "        ],\n",
    "        id='title',\n",
    "        className='title'),\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.Div(\n",
    "                # title above the sliders\n",
    "                [html.Div([\n",
    "                        html.Div([\n",
    "                            html.H6('Name')\n",
    "                        ],\n",
    "                        style={'width':'35%','text-decoration': 'underline'}),\n",
    "                        html.Div([\n",
    "                            html.H6('Slider')\n",
    "                        ],\n",
    "                        style={'width':'45%','text-decoration': 'underline'}),\n",
    "                        html.Div([\n",
    "                            html.H6('Value')\n",
    "                        ],\n",
    "                        style={'width':'10%','text-decoration': 'underline'}),\n",
    "                        html.Div([\n",
    "                            html.H6('Sx')\n",
    "                        ],\n",
    "                        style={'width':'10%','text-decoration': 'underline'}),\n",
    "                        html.Div([\n",
    "                            html.H6('Sy')\n",
    "                        ],\n",
    "                        style={'width':'10%','text-decoration': 'underline'})\n",
    "                    ],\n",
    "                id = 'index',\n",
    "                className = 'index')]+[\n",
    "                # cycle for for the 10 sliders\n",
    "                 html.Div([\n",
    "                    html.Div([\n",
    "                        html.H6(str(X_train.columns[indices[i]])+' : ', id = 'slider_variable_name_'+str(i))\n",
    "                        ],\n",
    "                        className='variable_name'),\n",
    "                    dcc.Slider(\n",
    "                        min = mins[indices[i]],\n",
    "                        max = maxs[indices[i]],\n",
    "                        value = query[indices[i]],\n",
    "                        step = steps[indices[i]],\n",
    "                        marks = select_ticks(mins[indices[i]], maxs[indices[i]], query[indices[i]], exp_input_values[indices[i]]),\n",
    "                        id = 'slider_'+str(i),\n",
    "                        className = 'slider'),\n",
    "                    html.Div(id = 'slider_'+str(i)+'_value', className = 'slider_value'),\n",
    "                    html.Div(id = 'slider_'+str(i)+'_xai_x', className = 'slider_value'),\n",
    "                    html.Div(id = 'slider_'+str(i)+'_xai_y', className = 'slider_value')\n",
    "                    ],\n",
    "                    id = 'variable_'+str(i),\n",
    "                    className = 'variable') for i in range(10)],\n",
    "                id='inputs',\n",
    "                className='inputs'),\n",
    "            html.Div([\n",
    "                dcc.Graph(\n",
    "                    id='space-plot',\n",
    "                    figure=fig)\n",
    "                ],\n",
    "                id='graph',\n",
    "                className='graph'),\n",
    "            ],\n",
    "            id='main_page',\n",
    "            className='main_page')\n",
    "        ],\n",
    "        id='main_body',\n",
    "        className='main_body'\n",
    "        )\n",
    "    ])\n",
    "\n",
    "#function to update the plot and the values to the 10 most important features\n",
    "@app.callback(\n",
    "    #Output 3 value for slider + the plot\n",
    "     sum([[dash.dependencies.Output('slider_'+str(i)+'_value', 'children'),\n",
    "      dash.dependencies.Output('slider_'+str(i)+'_xai_x', 'children'),\n",
    "      dash.dependencies.Output('slider_'+str(i)+'_xai_y', 'children'),\n",
    "     ] for i in range(10)],[])+[dash.dependencies.Output('space-plot', 'figure')],\n",
    "    # As input we only have the 10 value from the sliders\n",
    "    [dash.dependencies.Input('slider_'+str(i), 'value') for i in range(10)],\n",
    "    [dash.dependencies.State('space-plot', 'relayoutData')])\n",
    "def update_output(value0,value1,value2,value3,value4,value5,value6,value7,value8,value9,fig_data):\n",
    "    v = [value0,value1,value2,value3,value4,value5,value6,value7,value8,value9]\n",
    "    # query is the value to explain \n",
    "    # in this for loop change the value of the query according to the slider values\n",
    "    query = X_test.values[selected_point,:]\n",
    "    # Problem when selecting a new point: the slider values changes one at a time and this function is triggered 10 times!!! one for every changes with the older values\n",
    "    global change_point\n",
    "    if change_point:\n",
    "        if dash.callback_context.triggered[0]['prop_id'] != 'slider_9.value':\n",
    "            raise PreventUpdate\n",
    "        else: change_point = 0\n",
    "    for i in range(len(v)):\n",
    "        query[indices[i]] = v[i]\n",
    "    # compute xai values\n",
    "    exp_x, xai_x, exp_y, xai_y = compute_XAI_values(query)\n",
    "    #compute new z position\n",
    "    with torch.no_grad():\n",
    "        z , _ = FFNN_VAE_model.encode(torch.tensor(query).float())\n",
    "        z = z.numpy()\n",
    "    #update the scatter plot and the vector explanations\n",
    "    fig.for_each_trace(\n",
    "        lambda trace: trace.update(x=[z[0]], y=[z[1]]) if trace.name == \"pointer\" else (),\n",
    "        )\n",
    "    s = [exp_x, exp_y]\n",
    "    for i in range(len(v)):\n",
    "        #start\n",
    "        annotations[i]['x'] = s[0]\n",
    "        annotations[i]['y'] = s[1]\n",
    "        #end\n",
    "        s[0] += xai_x[indices[i]]\n",
    "        s[1] += xai_y[indices[i]]\n",
    "        annotations[i]['ax'] = s[0]\n",
    "        annotations[i]['ay'] = s[1]\n",
    "        annotations[i]['hovertext'] = str(X_train.columns[indices[i]])\n",
    "    annotations[-1]['x'] = s[0]\n",
    "    annotations[-1]['y'] = s[1]\n",
    "    annotations[-1]['ax'] = z[0]\n",
    "    annotations[-1]['ay'] = z[1]\n",
    "    # update with zoom level as selected by the user\n",
    "    try:\n",
    "        fig.update_layout({'xaxis':{'range':[fig_data['xaxis.range[0]'],fig_data['xaxis.range[1]']]},\n",
    "                           'yaxis':{'range':[fig_data['yaxis.range[0]'],fig_data['yaxis.range[1]']]},\n",
    "                          },\n",
    "                          annotations = annotations)\n",
    "    except: fig.update_layout(annotations = annotations)\n",
    "    # the output must be a list [slider_value,xai_x,xai_y]*10+[fig]\n",
    "    v = np.array(v)\n",
    "    xai_x = xai_x[indices]\n",
    "    xai_y = xai_y[indices]\n",
    "    out = list(np.vstack([v,xai_x,xai_y]).transpose().ravel())\n",
    "    # round the numbers\n",
    "    for i in range(len(out)):\n",
    "        if out[i]%1:\n",
    "            out[i] = np.round(out[i],3)\n",
    "        else: out[i] = int(out[i])\n",
    "    return out + [fig]\n",
    "    \n",
    "# select the point\n",
    "@app.callback(\n",
    "    sum([[dash.dependencies.Output('slider_variable_name_'+str(i), 'children'),\n",
    "      dash.dependencies.Output('slider_'+str(i), 'min'),\n",
    "      dash.dependencies.Output('slider_'+str(i), 'max'),\n",
    "      dash.dependencies.Output('slider_'+str(i), 'value'),\n",
    "      dash.dependencies.Output('slider_'+str(i), 'step'),\n",
    "      dash.dependencies.Output('slider_'+str(i), 'marks'),\n",
    "     ] for i in range(10)],[]),\n",
    "    dash.dependencies.Input('space-plot', 'clickData')\n",
    "    )\n",
    "def display_click_data(clickData):\n",
    "    try:\n",
    "        global selected_point\n",
    "        selected_point = clickData['points'][0]['pointIndex']\n",
    "        global change_point\n",
    "        change_point = 1\n",
    "        query = X_test.values[selected_point,:]\n",
    "        exp_x, xai_x, exp_y, xai_y = compute_XAI_values(query)\n",
    "        global fig\n",
    "        fig.for_each_trace(\n",
    "            lambda trace: trace.update(x=[exp_x], y=[exp_y]) if trace.name == \"expected_value\" else (),\n",
    "        )\n",
    "        s = [exp_x, exp_y]\n",
    "        #select only the 10 most important features\n",
    "        global indices\n",
    "        indices = np.argsort(np.sqrt(xai_x**2+xai_y**2))[-10:][::-1]\n",
    "        return sum([[str(X_train.columns[indices[i]])+' : ',mins[indices[i]],maxs[indices[i]],query[indices[i]],steps[indices[i]],select_ticks(mins[indices[i]], maxs[indices[i]], query[indices[i]], exp_input_values[indices[i]])] for i in range(10)],[])\n",
    "    except:\n",
    "        raise PreventUpdate\n",
    "    \n",
    "\n",
    "# run the app\n",
    "app.run_server(mode='external',\n",
    "               port=8090, \n",
    "               dev_tools_ui=True, \n",
    "               #debug=True,\n",
    "               dev_tools_hot_reload=False,\n",
    "               threaded=True,\n",
    "               #host='127.0.0.1:2595'\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-retro",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5943de3-7e8b-42f9-95cc-0fe91cef6ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a371e-f386-46b7-8058-94b7cb0939e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
